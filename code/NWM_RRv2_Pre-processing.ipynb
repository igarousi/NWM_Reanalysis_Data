{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the Retrieved CSV Files from NWM Retrospective NetCDF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook uses results from NWM_RRv2_Retrieve.py (i.e., zipped CSV files) to create single CSV for specified variables. For example, it reads NWM_2007_ALBEDO.zip, NWM_2008_ALBEDO.zip, ... and ouptuts NWM_ALBEDO.csv. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../output'                # The path including zipped cvs files\n",
    "output_dir = data_dir                 # The path to save final csv files\n",
    "#variables = ['ALBEDO', 'ALBSND', 'ALBSNI', 'COSZ', 'FIRA', \n",
    "#             'FSA', 'FSNO', 'HFX', 'LH', 'SNEQV', 'SNOWH', 'TRAD']\n",
    "variables = ['SNEQV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Manipulate NWM Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip NWM results and create a single CSV file for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in glob.glob(os.path.join(data_dir, \"*.zip\")):\n",
    "    \n",
    "    with zipfile.ZipFile(f, 'r') as ref:\n",
    "        \n",
    "        ref.extractall(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each variable, read all csv files and create a combined csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    \n",
    "    all_filenames = [i for i in glob.glob(os.path.join(data_dir, f'NWM_*_*_{v}.csv'))]\n",
    "    combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "    combined_csv.to_csv(os.path.join(output_dir, f'NWM_{v}.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Filter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells loop over all sites and for each day in the time period of interest, extract the values of snow water equivalent (LDASOUT outputs) or precipitation (FORCING iputs) as well as some other information related to the gage, and return a dataframe including 4 columns (Site_ID, Ecoregion_Name, Date_Time_UTC, and variale of interest) as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snotel_info = pd.read_csv(os.path.join(data_dir, 'SNOTEL_indices_at_NWM.csv')) \n",
    "\n",
    "nwm_p = pd.read_csv(os.path.join(data_dir, 'NWM_P.csv')) \n",
    "\n",
    "for v in variables:\n",
    "    vars()[f'nwm_{v}'] = pd.read_csv(os.path.join(data_dir, f'NWM_{v}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Define a Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_Hourly = pd.date_range('2007-10-01', '2018-10-02', freq='1H') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Define Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Filter_NWM(dataset1, dataset2, column_code, column_val, final_column_val, csv_name, output_dir):\n",
    "    \n",
    "    '''\n",
    "    date_range:        Period of interest     \n",
    "    dataset1:          Dataset for which Filter function is used  \n",
    "    dataset2:          Dataset including snotel information\n",
    "    column_code:       Column including station id\n",
    "    column_val:        Column including values of the variable of interest\n",
    "    final_column_val:  Column including values of the variable of interests as results \n",
    "    csv_name:          Name of the output (i.e., a CSV file)\n",
    "    output_dir:        Path to save outputs\n",
    "    '''\n",
    "    \n",
    "    # Get station ids\n",
    "#     code = set(dataset1[column_code])  # List of site codes \n",
    "    code = [1107, 1000, 823, 353, 669, 1127, 423, 376] \n",
    "    \n",
    "    ID = []\n",
    "    NAME = []\n",
    "    TIME = []\n",
    "    VALUE = []\n",
    "    for c in code:\n",
    "        print(c)\n",
    "        select = dataset1.loc[dataset1[column_code] == c]\n",
    "        for d in dates_Hourly:\n",
    "            try:\n",
    "                id = select[pd.to_datetime(select['time']) == d][column_code].values[0]\n",
    "                name = dataset2[dataset2[column_code] == c]['Ecoregion_NAME'].values[0]\n",
    "                value = select[pd.to_datetime(select['time']) == d][column_val].values[0]\n",
    "            except Exception as e:\n",
    "                value = np.nan\n",
    "            ID.append(id)\n",
    "            NAME.append(name)\n",
    "            TIME.append(d)\n",
    "            VALUE.append(value)\n",
    "\n",
    "    df = pd.DataFrame({'col1': ID, 'col2': NAME, 'col3': TIME, 'col4': VALUE})\n",
    "    df.columns = ['Site_Code',  'Ecoregion_Name', 'Date_Time_UTC', final_column_val] \n",
    "    df.index = df['Date_Time_UTC']\n",
    "    df.to_csv(os.path.join(output_dir, csv_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Apply Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107\n",
      "1000\n",
      "823\n",
      "353\n",
      "669\n",
      "1127\n",
      "423\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "# for v in variables:\n",
    "#     dataset1 = vars()[f'nwm_{v}']         \n",
    "#     dataset2 = vars()['snotel_info'] \n",
    "#     column_code = 'Station_ID'\n",
    "#     column_val = dataset1.columns[4]\n",
    "#     final_column_val = dataset1.columns[4]\n",
    "#     csv_name = f'NWM_{v}_Filter.csv'\n",
    "#     Filter_NWM(dataset1, dataset2, column_code, column_val, final_column_val, csv_name, data_dir)\n",
    "    \n",
    "# The first three commands modify some information within nwm_p. \n",
    "# This is becasue the file is prepared with the WRF-Hydro team members. \n",
    "# So, it does not have the same structure as SWE, which is created from our \n",
    "# developed scripts.\n",
    "dataset1 = vars()['nwm_p']       \n",
    "dataset1.rename(columns={'site_id':'Station_ID'}, inplace=True)                   # change the column name to be consistent with snotel_info\n",
    "dataset1['time'] = pd.to_datetime(dataset1['time'], format='%Y%m%d%H')            # make 'time' column as a data/time format \n",
    "Filter_NWM(vars()['nwm_p'] , vars()['snotel_info'], 'Station_ID', 'P', 'P_mm', 'NWM_P_Filter.csv', data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
